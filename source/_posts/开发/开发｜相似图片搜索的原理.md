---
title: 开发｜相似图片搜索的原理
date: 2023-07-12 00:04:38
tags: 算法
---
> <font color="gray">非常好奇，Google Lens（born in 2011）这类search by image应用背后的**原理**。  
> 去Google搜了一圈，没有搜到靠谱的解答，也许是我的搜索姿势不对？
>   
> 在中午互联网看到了阮一峰老大哥的解答(wrote in 2011)，真猛呐！～有空一定要好好学习阮大哥的其他博文（尤其是早年的）</font>  

先来看一个实例：  
- input:  <a href="https://www.ruanyifeng.com/blogimg/asset/201107/bg2011072103.jpg"><img src="https://www.ruanyifeng.com/blogimg/asset/201107/bg2011072103.jpg" width=15% height=15% style="display:block;margin:0 auto;"></a>

- output: <a href="https://www.ruanyifeng.com/blogimg/asset/201107/bg2011072104.jpg"><img src="https://www.ruanyifeng.com/blogimg/asset/201107/bg2011072104.jpg" width=40% height=40% style="display:block;margin:0 auto"></a>


我们用一张图片作为输入，计算机（搜索引擎）则接受输入，输出多张与输入图片**相似的**图片作为输出。  

先考虑最简单情形的处理（输入-输出 高度相似、无杂质干扰的情形）：  

我们使用**感知哈希算法（Percetual hash algorithm）**，它的核心步骤是：  
1. 每张图片生成一个**指纹(fingerprint)**<font color=gray>「实际上是个字符串」</font>
2. 比较不同图片的指纹，指纹越相近，就说明图片越相似。

----

具体实现：

**第一步，去细留粗（缩小尺寸）** 

将图片缩小到8px*8px的尺寸。这一步的目的是去除图片的细节，只保留结构、明暗等基本信息，摒弃不同图片间尺寸差异。

**第二步，简化色彩**

将缩小后的图片色彩，转为64级灰度。也就是说，所有像素点的色彩种类上限，为64种。

**第三步，计算灰度平均值**

计算所有像素（当前共64个像素）的灰度平均值。

**第四步，比较像素的灰度**

将每个像素的灰度，与图片整体的灰度平均值进行比较。若该像素的灰度大于或等于灰度平均值，记为1；反之则记为0.

**第五步，计算哈希值**

将上一步的比较结果，组合在一起，我们得到了一个64位的整数字符串，此即该图片的指纹。（需保证所有图片都采用相同的顺序组合像素点比较结果）

----

得到图片指纹后，就可以对比不同的图片，看看64位中有多少位是不一样的。在理论上，这等同于计算**汉明距离（Hamming distance）**。若不相同的数据位不超过5，就说明两张图片很相似（相似度超过了95.3%）。

以上算法的优点是简单快速，可以忽略色差、突出轮廓。缺点则是对图片局部称合度要求非常高。如果在图片上加几个文字，或将图片翻转，该算法的效能将大大下降。因此，该算法的最佳用途是根据略缩图，找出原图。

实际应用中，我们往往采用更强大的**pHash算法**和**SIFT算法**，它们能够识别图片的变形，只要变形幅度不超过25%，就能匹配原图。这些算法虽然更复杂，但是原理与我们刚刚所构建的简便算法是一致的，即先将图片转化为Hash字符串，然后再进行比较。

延伸了解：<a href="https://en.wikipedia.org/wiki/Otsu%27s_method">Otsu's method</a>

<font color=gray>
后记：

其实无论搜索的是图片还是文本或是视频......

原理都是将要搜的东西提取出特征值，然后按照特征值比较相似度，最后再按相似度排序将结果呈献给用户。

所以本质上来说，搜索，需要解决两个问题，一个是如何取得特征值，一个是如何计算相似度。
</font>